import numpy as np
import torch
import gym
import argparse
import os
import wandb
import tqdm
from ns3gym import ns3env
from ns3gym.start_sim import find_waf_path

from agents.our_ddpg.utils import ReplayBuffer
from agents.our_ddpg.preprocessor import Preprocessor
from agents.our_ddpg.Our_DDPG import DDPG
from agents.our_ddpg.loggers import Logger
from wrappers import EnvWrapper

def calculate_fitness(env, policy, replay_buffer, sim_args, preprocess, state_dim, action_dim, max_action, real_max_action, steps_per_ep, start_timesteps, batch_size):
    """
    Evaluates the fitness of a DDPG agent (wolf) based on accumulated rewards.
    """
    state, done = env.reset(), False
    total_reward = 0
    state = np.reshape(state[0][:state_dim], state_dim)

    for step in range(steps_per_ep):
        if step < start_timesteps:
            action = np.random.uniform(0, real_max_action, size=(1,))
        else:
            action = (
                policy.select_action(np.array(state)) +
                np.random.normal(0, max_action * args.expl_noise, size=action_dim)
            ).clip(-max_action, max_action)
            action = real_max_action * (action + 1) / 2

        next_state, reward, done, _ = env.step(np.array([action]))
        next_state = preprocess(next_state[0][:state_dim])
        replay_buffer.add(state, action, next_state, reward, done)

        policy.train(replay_buffer, batch_size)
        state = next_state
        total_reward += reward

        if done:
            break

    return total_reward

def update_positions(wolves, best_wolf, a, t, Tmax):
    """
    Update wolves' positions using GWO equations.
    """
    new_positions = []
    for wolf in wolves:
        A = 2 * a * np.random.random() - a
        C = 2 * np.random.random()
        D_alpha = abs(C * best_wolf - wolf)
        X1 = best_wolf - A * D_alpha

        D_beta = abs(C * np.random.random() - wolf)
        X2 = np.random.random() - A * D_beta

        D_delta = abs(C * np.random.random() - wolf)
        X3 = np.random.random() - A * D_delta

        new_positions.append((X1 + X2 + X3) / 3)
    return np.clip(new_positions, 0, 1)  # Ensure positions are valid

if __name__ == "__main__":
    # Initialize GWO parameters
    N = 5  # Number of wolves
    Tmax = 10  # GWO iterations
    wolves = np.random.rand(N, 5)  # Positions for (αactor, αcritic, γ, batch_size, τ)

    # Initialize DDPG parameters
    sim_args = {
        "simTime": 15,
        "envStepTime": 0.01,
        "historyLength": 300,
        "agentType": "continuous",
        "scenario": "basic",
        "nWifi": 50
    }
    threads_no = 1
    steps_per_ep = int(sim_args["simTime"] / sim_args["envStepTime"])
    state_dim = 1
    action_dim = 1
    max_action = 1
    real_max_action = 6
    start_timesteps = 300
    replay_buffer = ReplayBuffer(state_dim, action_dim)

    env = EnvWrapper(threads_no, **sim_args)
    preprocess = Preprocessor(False).preprocess

    best_fitness = -float("inf")
    best_wolf = None

    for t in range(Tmax):
        fitness_scores = []
        for i, wolf in enumerate(wolves):
            αactor, αcritic, γ, batch_size, τ = wolf

            # Initialize DDPG agent with wolf's hyperparameters
            kwargs = {
                "state_dim": state_dim,
                "action_dim": action_dim,
                "max_action": max_action,
                "discount": γ,
                "tau": τ,
                "actor_lr": αactor,
                "critic_lr": αcritic
            }
            policy = DDPG(**kwargs)

            # Evaluate fitness
            fitness = calculate_fitness(env, policy, replay_buffer, sim_args, preprocess, state_dim, action_dim, max_action, real_max_action, steps_per_ep, start_timesteps, int(batch_size))
            fitness_scores.append(fitness)

            if fitness > best_fitness:
                best_fitness = fitness
                best_wolf = wolf

        # Update wolves' positions
        a = 2 - t * (2 / Tmax)
        wolves = update_positions(wolves, best_wolf, a, t, Tmax)
        print(f"Iteration {t + 1}/{Tmax}, Best Fitness: {best_fitness:.2f}, Best Hyperparameters: {best_wolf}")

    print(f"Optimized Hyperparameters: {best_wolf}")

